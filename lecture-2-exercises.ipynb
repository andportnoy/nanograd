{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12713e74",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "\n",
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac25620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dca147",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f4b12",
   "metadata": {},
   "source": [
    "I do see an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd13a7",
   "metadata": {},
   "source": [
    "The goal is to implement a trigram model both using counts and a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87487005",
   "metadata": {},
   "source": [
    "How do we construct these trigrams? What is the idea if we use counts? A trigram is a sequence of three letters. We want to model the probability of seeing a particular letter given the previous two. How do we do that for the beginning of a word? Does a word begin with two '.' elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19180b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', 'r') as file:\n",
    "    words = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b529f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaefee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dtype = torch.float32\n",
    "torch.set_default_dtype(default_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b15c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams(words):\n",
    "    for w in words:\n",
    "        chs = ['.', '.'] + list(w) + ['.']\n",
    "        for c1, c2, c3 in zip(chs, chs[1:], chs[2:]):\n",
    "            yield c1, c2, c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4564de",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d447c0b",
   "metadata": {},
   "source": [
    "A trigram count model would map two chars to a single char that follows. What dimensions should a count lookup table have? Well, what are all the possible two char sequences that we might have? Certainly can be two dots or it can start with a dot. Can't start with a letter and end with a dot, because that should have terminated evaluation earlier. So the number should be $1\\cdot1 + 1\\cdot26 + 26\\cdot26 = 27\\cdot26 + 1 = 27\\cdot27-26$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4939aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOT = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd967f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "btoi = {}\n",
    "i = 0\n",
    "\n",
    "btoi[(DOT, DOT)] = i\n",
    "i += 1\n",
    "\n",
    "for c in chars:\n",
    "    btoi[(DOT, c)] = i\n",
    "    i += 1\n",
    "    \n",
    "for c1 in chars:\n",
    "    for c2 in chars:\n",
    "        btoi[(c1, c2)] = i\n",
    "        i += 1\n",
    "\n",
    "itob = {i: b for b, i in btoi.items()}\n",
    "\n",
    "ctoi = {}\n",
    "i = 0\n",
    "\n",
    "ctoi[DOT] = i\n",
    "i += 1\n",
    "\n",
    "for c in chars:\n",
    "    ctoi[c] = i\n",
    "    i += 1\n",
    "    \n",
    "itoc = {i: c}\n",
    "\n",
    "itoc = {i: c for c, i in ctoi.items()}\n",
    "\n",
    "m = len(itob)\n",
    "n = len(itoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097711c5",
   "metadata": {},
   "source": [
    "# Count-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((m, n), dtype=torch.int32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db74cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c1, c2, c3 in trigrams(words):\n",
    "    i1 = btoi[(c1, c2)]\n",
    "    i2 = ctoi[c3]\n",
    "    N[i1, i2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()\n",
    "P /= P.sum(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27393bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device='cpu').manual_seed(42)\n",
    "\n",
    "def makeone():\n",
    "    i = 0\n",
    "    s = ''\n",
    "    while True:\n",
    "        p = P[i]\n",
    "        ci = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        # now we have a new character index, we need to update our lookup bigram\n",
    "        # the new lookup bigram will contain the second character in the first position\n",
    "        # and the new character in the second position\n",
    "        # what is the current bigram? it is given by i\n",
    "        if ci == 0:\n",
    "            break\n",
    "        i = btoi[(itob[i][1], itoc[ci])]\n",
    "        s += itoc[ci]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282df351",
   "metadata": {},
   "source": [
    "Now want to evaluate the model. The idea is to calculate the likelihood of the dataset given the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = 0\n",
    "k = 0\n",
    "for word in words:\n",
    "    chs = ['.', '.'] + list(word) + ['.']\n",
    "    for c1, c2, c3 in zip(chs, chs[1:], chs[2:]):\n",
    "        i1 = btoi[(c1, c2)]\n",
    "        i2 = ctoi[c3]\n",
    "        ll += torch.log(P[i1, i2])\n",
    "        k += 1\n",
    "print(f\"mean negative likelihood is {-ll/k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb4325",
   "metadata": {},
   "source": [
    "# Gradient descent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c37f0",
   "metadata": {},
   "source": [
    "Create a dataset. We do this by turning character indices into one hot vector. The first character in a bigram is an $x$, the second character is a $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1696a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for word in words:\n",
    "    chs = ['.', '.'] + list(word) + ['.']\n",
    "    for c1, c2, c3 in zip(chs, chs[1:], chs[2:]):\n",
    "        i1 = btoi[(c1, c2)]\n",
    "        i2 = ctoi[c3]\n",
    "        xs.append(i1)\n",
    "        ys.append(i2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, m).to(default_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device='cuda').manual_seed(2147483647)\n",
    "W = torch.randn(m, n, generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "print(f\"{'epoch':>6} {'loss':>10} {'time,s':>7}\")\n",
    "for i in range(10000+1):\n",
    "    # --- ONE-HOT ---\n",
    "    logits = xenc @ W # log-counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(xs)), ys].log().mean()# + 20*(W**2).mean()\n",
    "    if (i+1)%1000 == 1:\n",
    "        tt = time.time()\n",
    "        print(f\"{i+1:6} {loss.data.item():10.5f} {tt-t:>7.2f}\")\n",
    "        t = tt\n",
    "\n",
    "    W.grad = None # zero out the gradients\n",
    "    loss.backward()\n",
    "    W.data += -50*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "s = ''\n",
    "while True:\n",
    "    xenc = F.one_hot(torch.tensor([i]), m).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    ci = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "    if ci == 0:\n",
    "        break\n",
    "    i = btoi[(itob[i][1], itoc[ci])]\n",
    "    s += itoc[ci]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc44b62",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e3ffd",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98c4dd",
   "metadata": {},
   "source": [
    "So to be precise, I want to train and evaluate bigram/trigram count/neural models. Can I encapsulate all these models behind a common interface to make it easier to train and evaluate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809512f",
   "metadata": {},
   "source": [
    "What should that interface be? I want to be able to 1. train 2. evaluate. In 1. I want to pass in an X and a Y nd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108231cc",
   "metadata": {},
   "source": [
    "Ok, let's build a count based bigram model class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646679e9",
   "metadata": {},
   "source": [
    "Could either split at the word level, or at the character-character/bigram-character level. I should design the classes such that I don't have to worry about that. That is, the functions should operate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998608fd",
   "metadata": {},
   "source": [
    "I think the count based models boil down to lookup tables. So we should call them lookup models of certain dimensions, and factor out character to index conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOT = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountModel:\n",
    "    def __init__(self, m, n):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        assert len(X.shape) == 1\n",
    "        assert len(Y.shape) == 1\n",
    "        assert X.max() < self.m\n",
    "        assert Y.max() < self.n\n",
    "        self.N = torch.zeros((self.m, self.n), dtype=torch.int32)\n",
    "        for i1, i2 in zip(X, Y):\n",
    "            self.N[i1, i2] += 1\n",
    "        self.P = self.N.float() + 0.000001\n",
    "        self.P /= self.P.sum(axis=1, keepdim=True)\n",
    "        \n",
    "    def eval(self, X, Y):\n",
    "        assert len(X.shape) == 1\n",
    "        assert len(Y.shape) == 1\n",
    "        assert X.max() < self.m\n",
    "        assert Y.max() < self.n\n",
    "        ll = sum(torch.log(self.P[i1, i2]) for i1, i2 in zip(X, Y))\n",
    "        return (-ll/len(X)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d4ba9",
   "metadata": {},
   "source": [
    "Let's just implement what's necessary and package later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7b444",
   "metadata": {},
   "source": [
    "The pipeline in the bigram case is:\n",
    "Words -> bigrams -> index pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigrammer:\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.chars = sorted(list(set(''.join(self.words))))\n",
    "        self.ctoi = {s: i+1 for i, s in enumerate(self.chars)}\n",
    "        self.ctoi[DOT] = 0\n",
    "        self.itoc = {i: s for s, i in self.ctoi.items()}\n",
    "        self.m = len(self.itoc)\n",
    "        self.n = self.m\n",
    "        \n",
    "    def bigrams(self):\n",
    "        for w in self.words:\n",
    "            chs = [DOT] + list(w) + [DOT]\n",
    "            for c1, c2 in zip(chs, chs[1:]):\n",
    "                yield c1, c2\n",
    "                \n",
    "    def indexpairs(self):\n",
    "        for c1, c2 in self.bigrams():\n",
    "            i1 = self.ctoi[c1]\n",
    "            i2 = self.ctoi[c2]\n",
    "            yield i1, i2\n",
    "            \n",
    "    def xy(self):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i1, i2 in self.indexpairs():\n",
    "            X.append(i1)\n",
    "            Y.append(i2)\n",
    "        X = torch.tensor(X)\n",
    "        Y = torch.tensor(Y)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8ba55",
   "metadata": {},
   "source": [
    "Now let's make a neural network model for bigrams. Actually it will work for trigrams as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03225b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralModel:\n",
    "    def __init__(self, m, n, device='cpu', seed=2147483647):\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        self.W = torch.randn(m, n, generator=g).to(device)\n",
    "        self.W.requires_grad = True\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.device = device\n",
    "    \n",
    "    def _forward(self, X, Y):\n",
    "        logits = X @ self.W # log-counts\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        loss = -probs[torch.arange(len(X)), Y].log().mean()# + 20*(W**2).mean()\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, Y, epochs=1000, verbose=False, every=100, lr=1):\n",
    "        assert len(X.shape) == 1\n",
    "        assert len(Y.shape) == 1\n",
    "        assert X.max() < self.m\n",
    "        assert Y.max() < self.n\n",
    "        X = F.one_hot(X, self.m).float()\n",
    "        X = X.to(self.device)\n",
    "        Y = Y.to(self.device)\n",
    "        if verbose:\n",
    "            print(f\"{'epoch':>6} {'loss':>10} {'time,s':>7}\")\n",
    "        loss = self._forward(X, Y)\n",
    "        if verbose:\n",
    "            print(f\"{0:6} {loss.data.item():10.5f}\")\n",
    "        start = time.time()\n",
    "        for i in range(epochs):\n",
    "            self.W.grad = None # zero out the gradients\n",
    "            loss.backward()\n",
    "            self.W.data += -lr*self.W.grad\n",
    "            loss = self._forward(X, Y)\n",
    "            if verbose and (i==epochs-1 or (i+1)%every == 0):\n",
    "                end = time.time()\n",
    "                print(f\"{i+1:6} {loss.data.item():10.5f} {end-start:>7.2f}\")\n",
    "                start = end\n",
    "\n",
    "    def eval(self, X, Y):\n",
    "        assert len(X.shape) == 1\n",
    "        assert len(Y.shape) == 1\n",
    "        assert X.max() < self.m\n",
    "        assert Y.max() < self.n\n",
    "        X = F.one_hot(X, self.m).float()\n",
    "        X = X.to(self.device)\n",
    "        Y = Y.to(self.device)\n",
    "        loss = self._forward(X, Y)\n",
    "        return loss.item()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3027e7",
   "metadata": {},
   "source": [
    "Now just need to make a trigrammer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trigrammer:\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        chars = sorted(list(set(''.join(self.words))))\n",
    "        \n",
    "        self.btoi = {}\n",
    "        i = 0\n",
    "        self.btoi[(DOT, DOT)] = i\n",
    "        i += 1\n",
    "        for c in chars:\n",
    "            self.btoi[(DOT, c)] = i\n",
    "            i += 1\n",
    "        for c1 in chars:\n",
    "            for c2 in chars:\n",
    "                self.btoi[(c1, c2)] = i\n",
    "                i += 1\n",
    "\n",
    "        self.itob = {i: b for b, i in self.btoi.items()}\n",
    "\n",
    "        self.ctoi = {}\n",
    "        i = 0\n",
    "        self.ctoi[DOT] = i\n",
    "        i += 1\n",
    "        for c in chars:\n",
    "            self.ctoi[c] = i\n",
    "            i += 1\n",
    "        self.itoc = {i: c}\n",
    "        self.itoc = {i: c for c, i in self.ctoi.items()}\n",
    "\n",
    "        self.m = len(self.itob)\n",
    "        self.n = len(self.itoc)\n",
    "        \n",
    "    def trigrams(self):\n",
    "        for w in self.words:\n",
    "            chs = ['.', '.'] + list(w) + ['.']\n",
    "            for c1, c2, c3 in zip(chs, chs[1:], chs[2:]):\n",
    "                yield c1, c2, c3\n",
    "                \n",
    "    def indexpairs(self):\n",
    "        for c1, c2, c3 in self.trigrams():\n",
    "            i1 = self.btoi[(c1, c2)]\n",
    "            i2 = self.ctoi[c3]\n",
    "            yield i1, i2\n",
    "            \n",
    "    def xy(self):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i1, i2 in self.indexpairs():\n",
    "            X.append(i1)\n",
    "            Y.append(i2)\n",
    "        X = torch.tensor(X)\n",
    "        Y = torch.tensor(Y)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ca433",
   "metadata": {},
   "source": [
    "How do I split? I can split into arbitrarily many subsets if I can split into two of arbitrary sizes. E.g. first split into 80 and 20, then split the 20 in half."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb68649",
   "metadata": {},
   "source": [
    "I need to split both xs and ys. How? Can I merge them into a single tensor first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x, device='cpu'):\n",
    "\n",
    "    g=torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    n = len(x)\n",
    "    indices = torch.randperm(n, generator=g)\n",
    "\n",
    "    a, b = 0, round(0.8 * n)\n",
    "    train = x[indices[a:b]]\n",
    "\n",
    "    a, b = b, b+round(0.1*n)\n",
    "    test = x[indices[a:b]]\n",
    "\n",
    "    a, b = b, n\n",
    "    dev = x[indices[a:b]]\n",
    "    \n",
    "    return train, test, dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, train, test, dev, **kwargs):\n",
    "    Xtrain, Ytrain = train\n",
    "    Xtest, Ytest = test\n",
    "    Xdev, Ydev = dev\n",
    "    model.train(Xtrain, Ytrain, **kwargs)\n",
    "    results = {\n",
    "        'train': model.eval(Xtrain, Ytrain),\n",
    "        'test': model.eval(Xtest, Ytest),\n",
    "        'dev': model.eval(Xdev, Ydev)\n",
    "    }\n",
    "    for name, result in results.items():\n",
    "        print(f\"{name:5}: {result:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffd4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Bigrammer(words)\n",
    "X2, Y2 = b.xy()\n",
    "XY2 = torch.stack((X2, Y2), dim=1)\n",
    "train2, test2, dev2 = (x.permute((1, 0)) for x in split(XY2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36df5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(CountModel(b.m, b.n), train2, test2, dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(NeuralModel(b.m, b.n), train2, test2, dev2, lr=50, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b86409",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Trigrammer(words)\n",
    "X3, Y3 = t.xy()\n",
    "XY3 = torch.stack((X3, Y3), dim=1)\n",
    "train3, test3, dev3 = (x.permute((1, 0)) for x in split(XY3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(CountModel(t.m, t.n), train3, test3, dev3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(NeuralModel(t.m, t.n), train3, test3, dev3, lr=50, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa07f3",
   "metadata": {},
   "source": [
    "In all cases the model does worse on the test/dev subsets than on the train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
