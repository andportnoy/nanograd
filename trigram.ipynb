{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac25620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd13a7",
   "metadata": {},
   "source": [
    "The goal is to implement a trigram model both using counts and a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87487005",
   "metadata": {},
   "source": [
    "How do we construct these trigrams? What is the idea if we use counts? A trigram is a sequence of three letters. We want to model the probability of seeing a particular letter given the previous two. How do we do that for the beginning of a word? Does a word begin with two '.' elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a93965d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b529f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbaefee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dtype = torch.float32\n",
    "torch.set_default_dtype(default_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae294a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', 'r') as file:\n",
    "    words = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b15c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams(words):\n",
    "    for w in words:\n",
    "        chs = ['.', '.'] + list(w) + ['.']\n",
    "        for c1, c2, c3 in zip(chs, chs[1:], chs[2:]):\n",
    "            yield c1, c2, c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4564de",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d447c0b",
   "metadata": {},
   "source": [
    "A trigram count model would map two chars to a single char that follows. What dimensions should a count lookup table have? Well, what are all the possible two char sequences that we might have? Certainly can be two dots or it can start with a dot. Can't start with a letter and end with a dot, because that should have terminated evaluation earlier. So the number should be $1\\cdot1 + 1\\cdot26 + 26\\cdot26 = 27\\cdot26 + 1 = 27\\cdot27-26$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4939aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOT = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd967f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "btoi = {}\n",
    "i = 0\n",
    "\n",
    "btoi[(DOT, DOT)] = i\n",
    "i += 1\n",
    "\n",
    "for c in chars:\n",
    "    btoi[(DOT, c)] = i\n",
    "    i += 1\n",
    "    \n",
    "for c1 in chars:\n",
    "    for c2 in chars:\n",
    "        btoi[(c1, c2)] = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c252f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "itob = {i: b for b, i in btoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af526a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi = {}\n",
    "i = 0\n",
    "\n",
    "ctoi[DOT] = i\n",
    "i += 1\n",
    "\n",
    "for c in chars:\n",
    "    ctoi[c] = i\n",
    "    i += 1\n",
    "    \n",
    "itoc = {i: c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81468592",
   "metadata": {},
   "outputs": [],
   "source": [
    "itoc = {i: c for c, i in ctoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66003b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(itob)\n",
    "n = len(itoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097711c5",
   "metadata": {},
   "source": [
    "# Count-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3389ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((m, n), dtype=torch.int32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5db74cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c1, c2, c3 in trigrams(words):\n",
    "    i1 = btoi[(c1, c2)]\n",
    "    i2 = ctoi[c3]\n",
    "    N[i1, i2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f28a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()\n",
    "P /= P.sum(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27393bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device='cpu').manual_seed(42)\n",
    "\n",
    "def makeone():\n",
    "    i = 0\n",
    "    s = ''\n",
    "    while True:\n",
    "        p = P[i]\n",
    "        ci = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        # now we have a new character index, we need to update our lookup bigram\n",
    "        # the new lookup bigram will contain the second character in the first position\n",
    "        # and the new character in the second position\n",
    "        # what is the current bigram? it is given by i\n",
    "        if ci == 0:\n",
    "            break\n",
    "        i = btoi[(itob[i][1], itoc[ci])]\n",
    "        s += itoc[ci]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282df351",
   "metadata": {},
   "source": [
    "Now want to evaluate the model. The idea is to calculate the likelihood of the dataset given the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d1b247b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean negative likelihood is 2.185652017593384\n"
     ]
    }
   ],
   "source": [
    "ll = 0\n",
    "k = 0\n",
    "for word in words:\n",
    "    chs = ['.', '.'] + list(word) + ['.']\n",
    "    for c1, c2, c3 in zip(chs, chs[1:], chs[2:]):\n",
    "        i1 = btoi[(c1, c2)]\n",
    "        i2 = ctoi[c3]\n",
    "        ll += torch.log(P[i1, i2])\n",
    "        k += 1\n",
    "print(f\"mean negative likelihood is {-ll/k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb4325",
   "metadata": {},
   "source": [
    "# Gradient descent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c37f0",
   "metadata": {},
   "source": [
    "Create a dataset. We do this by turning character indices into one hot vector. The first character in a bigram is an $x$, the second character is a $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1696a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for word in words:\n",
    "    chs = ['.', '.'] + list(word) + ['.']\n",
    "    for c1, c2, c3 in zip(chs, chs[1:], chs[2:]):\n",
    "        i1 = btoi[(c1, c2)]\n",
    "        i2 = ctoi[c3]\n",
    "        xs.append(i1)\n",
    "        ys.append(i2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2da8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, m).to(default_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55cc22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device='cuda').manual_seed(2147483647)\n",
    "W = torch.randn(m, n, generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa0c02",
   "metadata": {},
   "source": [
    "### Train using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47dee18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch       loss  time,s\n",
      "     1    3.75721    0.39\n",
      "  1001    2.23654    2.64\n",
      "  2001    2.21365    2.70\n",
      "  3001    2.20539    2.70\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "print(f\"{'epoch':>6} {'loss':>10} {'time,s':>7}\")\n",
    "for i in range(3000+1):\n",
    "    # --- ONE-HOT ---\n",
    "    logits = xenc @ W # log-counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(xs)), ys].log().mean()# + 20*(W**2).mean()\n",
    "    if (i+1)%1000 == 1:\n",
    "        tt = time.time()\n",
    "        print(f\"{i+1:6} {loss.data.item():10.5f} {tt-t:>7.2f}\")\n",
    "        t = tt\n",
    "\n",
    "    W.grad = None # zero out the gradients\n",
    "    loss.backward()\n",
    "    W.data += -50*W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d31ee",
   "metadata": {},
   "source": [
    "### Train using indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "039d5523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch       loss  time,s\n",
      "     1    2.20538    0.00\n",
      "  1001    2.20098   20.02\n",
      "  2001    2.19820   20.38\n",
      "  3001    2.19626   20.38\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "print(f\"{'epoch':>6} {'loss':>10} {'time,s':>7}\")\n",
    "for i in range(3000+1):\n",
    "    # --- INDEXING ---\n",
    "    logits = W[xs] # log-counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(xs)), ys].log().mean()# + 20*(W**2).mean()\n",
    "    if (i+1)%1000 == 1:\n",
    "        tt = time.time()\n",
    "        print(f\"{i+1:6} {loss.data.item():10.5f} {tt-t:>7.2f}\")\n",
    "        t = tt\n",
    "\n",
    "    W.grad = None # zero out the gradients\n",
    "    loss.backward()\n",
    "    W.data += -50*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "241b933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aarthusia\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "s = ''\n",
    "while True:\n",
    "    xenc = F.one_hot(torch.tensor([i]), m).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    ci = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "    if ci == 0:\n",
    "        break\n",
    "    i = btoi[(itob[i][1], itoc[ci])]\n",
    "    s += itoc[ci]\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
